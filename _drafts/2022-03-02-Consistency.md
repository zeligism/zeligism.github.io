---
layout: post
title:  "Consistency"
date:   2022-03-02 02:18:19 +0400
---

The idea of a world view in AI is very important. If you want to simulate an autonomous agent that interacts with the world in a non-trivial manner, then we can say that the agent has to maintain a “world-view”.

#### World-view
This world-view should contain as much info about the world as possible in order to be efficient---maybe even the entire history of the observations of the world in case memory is limitless. However, not only is memory storage a constraint, but so are memory access and computing power. If memory is limitless somehow, then can we really access it efficiently? Can we take a memory and process it within the agent in a meaningful duration of time? It might be theoritcally possible using some abstract mathematical model that we are not aware of. Assuming memory storage is finite, memories can be further compressed and indexed as they are stored. Compression is obviously an advantage when memory is finite, and indexing is natural given that we need to access memories somehow (and not necessarily access them exactly). So, how are memories compressed and indexed, then? One thing for sure is that context can play a key role in these two processes. I believe an important part of memory (all of it?) is stored in a sequential or programmatic manner, i.e. large memories are compressed as the output of much shorter programs. This way, you can compress infinite strings into a finite program (i.e. algorithm). Heck, there is no such thing as an infinite string that cannot be stored as the output of some program. To see how this is true, think about how you would describe writing your infinite string. Then, your description itself is the program (or the constraints of an implied program understood from context) that generates this infinite string. This is an interesting observation because we could imagine an infinite string that is indescribable in finite words or self-destructive in some sense (self-contradictory, for example), but remember that such strings are not “computable” so they are not the output of a meaningful program. (I’m not well-versed in theoretical computer science, so I could be wrong.)

#### Programs
When you introduce programs, you will then have to introduce interpreters because the same program can be interpreted differently by different agents. When it comes to language, the interpretation is up to the community of agents that is using it. Language construction and interpretation is distributed in some sense; it is not local to a single agent and it evolves as a whole within all the participating agents together as there is no single truth to language interpretation. This allows language to be stable and well-interpretable among the majority of agents without having a central authority on language. This could be the case under some assumptions, of course. It could be the case because knowledge about language is communicated regularly among all of the agents and large variations are usually suppressed (frowned upon, actually) or just slightly adjusted on the go. Even though adjustments are local to the agent (the feedback comes only from the neighboring agents in the communication network), the agents could potentially converge to approximately the same knowledge at the end. (I don’t have a proof for that, but I believe it’s possible to prove it given some additional technical assumptions. For example, one assumption could be the maintenance of a grammar by the community using that language.) In the same manner, we can have a distributed kind of memory that can process some form of a program description in a distributed way, and then generate outputs independently, verify the outputs, and then adjust interpretation through consensus. The interpretation should be able to reconstruct the relevant parts of the sensory inputs (or an intermediary description of it) as perfectly as possible. This compression / interpretation process can be hierarchical such that it is multi-step and intermediary results can be used in other processes. Once the agents reach a stable compression-interpretation scheme, they can store memories as accurately as possible in the shortest form possible. What remains is accessing this particular memory through an index.

#### It's All a Dream
Indexing is the context-relevant part of the story. When we say “index”, we’re not really talking about a number, or a memory address. We’re talking about a signal that evokes a memory in a smooth manner given some memory state (slightly changing the signal given that memory state can still evoke similar kinds of memories). The signal could be a picture of a scene that you saw or a specific tone you heard during that memory you want to retrieve. Recalling a memory is not all obvious, as we seem to be able to recall memories without a reconstructable internal brain activity/signal. I might be wrong, especially if that internal signal could be exactly described somehow by the person, in which case there does exist a reconstructable brain signal (think about a neuron that represents a very specific property). Recalling a memory is similar to a program that runs in the brain—a search program, to be specific—so it is not all that different from other “programs” running in the brain. So far, terms like “program”, “description” , and “algorithm” seem to be describing the same phenomenon in the brain. The brain has an automatic recall feature, where a sensation can involuntarily remind you of something correlated with it, potentially completely irrelevant or just relevant to either the sensation or the context or their combination. This feature can be exploited to perform a sort of a progressively-selective indexing routine. We can start with a seed signal (a sensation, a thought, or a memory) and start actively recalling memories from that seed, and then select the relevant memories and try to recall finer details in the same fashion iteratively until we converge to what we believe to be the target memory we want to recall (which might not necessarily be an actual memory that we had but a made up one that was formed through a brainwash. You first have to believe that you can recall the target memory—that it is in your brain— given some sort of a “meta-description/signal” of it, which can be vague or wrong. You can’t recall a memory that didn’t happen, so you generate it and thus know that it is fake. But wouldn’t recalling a memory that have indeed happened be the same? Don’t we simply “re-generate” actual memories while assuming that they are real?)

#### Guts
This convergence process is clearly not as simple as it sounds as there are no guarantees that it can happen in finite time given the conditions described. One way we can enforce convergence to this recall process is to evaluate the relevance of the memory we recalled. The memory could be very relevant, and may as well feel like it’s true in the sense that it happened---or is just true by gut feeling. This “gut feeling”, I believe, is the qualia byproduct of the evaluation mechanism that estimates the convergence of the recall process, or simply estimates the truthiness of a thought. If your gut feeling is a conflicted and uneasy feeling, then you’re brain is basically saying that you haven’t converged to a stable memory or state of mind given the query you’re evaluating (recalling a memory or resolving the meaning and implication of a particular statement, for instance.) A strong gut feeling will tell you that you have a strong instinctive reason to believe in the result of your query. The query is the source of conflict that was introduced either by yourself or by an external source of information. Further explaining and understanding the emergence of this gut feeling, beyond the fact that it feels as elementary as the sense of a smell or a color, is completely out of my expertise. But the creation of a query that provokes gut feeling could probably be only associated with (or evoked by) native instinctive needs such as curiosity, survival, reproduction, etc. These instinctive needs could have probably been achieved via evolution---or perhaps by something more structured and elaborate. Who knows.

#### Time Devours Life
Ok, so far, we have tried to describe the concept of a world-view in an intelligent agent and how the agent can access information from it, but we haven’t yet explained anything about the dynamics of this procedure. For example, how does the world-view changes with each access or with time? Can we assume that the brain has the same structure and that it processes two consecutive “moments” in the same exact manner? The brain does have the ability to tell which moment came before, what happened and what didn’t, what might happen, and what might have happened if something else happened, which is like peeking into another timeline where such a thing did actually happen. But I believe that the brain does not have a “clock”, so to speak. It only creates chains of events that are marked by their temporal positions with respect to some significant event of which the date or timestamp had been explicitly memorized (e.g. your birth, your school years, etc.). Sometimes, it’s difficult to recall which year a particular event happened in your life, or whether one event happened before the other (think about what year in college you took a particular course, or how fast time moves in some scenarios vs. how slow it seems to move in others). The temporal order is only relevant when there is an interesting pattern. Who cares whether you yawned or scratched your head and whether you did it before brushing your teeth or after before sleeping. These are very trivial events that could be totally forgotten about and perhaps even believed to have not happened at all when asked about---let alone give their temporal order---and, surely, absolutely nothing will change in case you remembered the temporal order of these events, unless someone recorded that information and decided to quiz you on it for a million dollar (damn, that would totally suck). Seems like time is only important in the sense that it can only be “consumed” by “going forward”, and what is left behind in the back is something that “happened” for sure, regardless of which came before which (that would be figured out by the brain if it had been important). Thinking about it poetically, it is like time consumes the future and generates observables, which are the only things that we know that “are” for sure. Everything else is uncertain in this world. Even the things that we observe are only as good as our observations themselves, whether they are sense data or measurements. It is much like the process of discovering a mathematical theorem. I mean, that’s why it’s called “discovering a theorem” because it was already there and we just merely observed that it is consistent with our axioms.

#### Consistency
Now if you connect the whole thing together and try to squeeze it all into one grand picture, you do get some fundamental ideas behind the maintenance of a world-view. We said that memories could be seen as distributed programs that take a seed signal or a description and produce a sequence of signals. The signals converge if the memory exists and can be recalled, where convergence is evaluated by gut feeling. Also, there is no explicit temporal ordering in this world-view. The sequence of signals produced in this process dictates the temporal order, not the other way around. So what do we take from that? Probably the fact that “consistency” is key. Consistency is paramount to world-view, probably for the same reason that it is for math, science, and any other discipline that is based on axiomatic principles. Without consistency, memories cannot be compressed well and cannot be recalled reliably. Also, without consistency, temporal order would be distorted and probably collapse. So what we can take away is that the world-view is maintained by minimizing inconsistencies as much as possible, where inconsistencies can be succinctly defined as “contradictory observations”, e.g. observations that say that such and such has the property of being something and not being it. These are usually found in the edge cases, where prior knowledge have to be refined in order to maintain consistency. Inconsistencies can be dealt with either by deleting one observation---acknowledging the fact that we observed wrongly---or by “adding”  details to the observations (e.g. adding conditions, “something is X at one moment, but could be Y at another”). Proceeding like this, our world-view becomes more and more consistent, allowing us to have a better expectation of the future (which might be one of the main objectives of the brain). This is assuming we live in a stable, consistent world (well, as long we have science and not magic, I guess we do).

#### “Self”-Consistency
We still haven’t attempted to explain the way agents should interact with the world given this world-view, and why the heck people have different world-views if the process of maintaining one is as simple as “eliminate inconsistencies”. Well, to answer the second question first, it is because of the gut feeling-like factors that are controlled by genetics, as well as the process of inconsistency elimination itself. People can eliminate inconsistencies differently, and sometimes even still achieve the same world-view. So the interesting question is how agents interact with the world. This is a very interesting question, and it is perhaps the crux of this essay, despite being not so central to the idea of a world-view per se. This is what I believe:
**Agents interact with the world in a manner that maintains their world-view consistency**.
There is nothing particularly deep about that statement, I know, but I do think it’s deep. The reason why I think it’s deep is because in this framework, it seems like the agent’s identity is intertwined with the world’s identity, so the agent sees itself as a “thing” in this world: a thing which it could completely control, much like peeping into the world through a spiritual hole which is the “self”, or, put simply, “playing a video game”. The existence of the source of decisions and the engine of our interactions can literally be anywhere. It only needs this channel which is somewhere in the brain. It might even be the case that all consciousnesses exist in a realm where they are somehow interconnected. Sure, that’s stepping into sci-fi territory, but prove me wrong. Looking at it this way, you start to feel kinda detached from your worldly “self”. The brain has full control of what happens in the world, but doesn’t have full control of its inputs---the inputs that come from that thing over there in the realm of “free will”, consciousness, and whatnot. You know what I mean?

#### Rewards, Curiosity, Etc.
Anyway, this is kinda bullshit. We know that the brain’s behavior can be explained well by other simple concepts such as reward-maximization, curiosity, etc. But in my opinion, these can still be described in the aforementioned framework. Curiosity can be a part of the inconsistency elimination process given that the agent have the ability to generate elements in its world-view that were not observed before per se, and thus the agent might feel like it has the need to verify the modified---but in some parts imaginary---world-view that it now has. Also, I think that reward-maximization is a subset of inconsistency elimination. Reward-maximization is a special case when the agent sees that it wants the rewards, but it is not a special case when the agent might see itself as not deserving of the reward or avoiding it for some psychological reason (and thus some rewards could be conditioned by the self to paradoxically evoke negative feelings as well). In fact, in most cases, inconsistency elimination and reward-maximization are synonymous to each.

#### \~
In the end, I think that what the agent wants depends on genetic needs as well as goals created through the process of maintaining the consistency of a world-view. This is the agent-driven (internal) dynamics. As for the environment-driven (external) dynamics, lower total energy along a trajectory is preferred, which gives rise to conciseness. Thus, the intelligent agent will minimize the extent in which the constraints of the environment are violated (consuming the lowest energy) while maximizing the consistency of its world-view (achieving best accuracy). Consistency and conciseness might even be two faces of the same coin.
