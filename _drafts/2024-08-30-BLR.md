---
layout: post
title:  The Bayesian Learning Rule
date:   2023-01-01 00:00:00 +0000
academic: true
---

This is my first (and probably longest) blog post.
The first section is a casual introduction to gradient descent, the one we all know and love, and mirror descent, the hero in disguise.
This will serve as a motivation and will give some intuition for the next section, which is about the Bayesian Learning Rule (BLR) by [Khan & Rue (2023)](https://arxiv.org/abs/2107.04562).
This is a recent paper that compiles many results from previous work Khan et al. under one framework, which I think is beatiful and worth learning about.
It is the most elegant connection I have seen between optimization and Bayesian inference.
It is reminiscent of the connection between the MAP estimator of linear regression given a Gaussian prior and ridge regression,
but much more general.
There is some math involved, but a reader can get away from skipping it and reading about the intuition behind it.
Overall, the BLR is a new perspective on optimization that can shed light on some practices and help design better and more specialized algorithms.

<br>

---

<br>

# Descending a valley, but you're blindfolded
<!-- Helper math commands (hide them after the first section) -->
$\newcommand{\E}{\mathbb{E}}$
Given a point $x_t \in \mathcal{X}$ for iteration $t$ (your current location in the valley), a loss function $f : \mathcal{X}\to\mathbb{R}$ (the terrain's topology), and a step size $\eta_t$ (the length of your stick squared), a gradient (slope) descent step can be formulated as the solution of following sub-problem: find the lowest point of a linear approximation of $f$ around $x_0$ that is at most $\sqrt{2\eta_t}$ away (i.e., check the lowest point with your stick and go there)


$$
\begin{equation}
    x_{t+1} = \arg\min_{x} \ f(x_t) + \langle x - x_t, \nabla f(x_t) \rangle, \quad\text{s.t.}\ \|x-x_t\| \leq \sqrt{2\eta_t},
\end{equation}
$$

or, equivalently, find the minimizer of a "second-order" approximation of $f$ around $x_t$ assuming that $\nabla^2 f(x_t) \approx \frac{1}{2\eta_t}I$

$$
\begin{equation}
    x_{t+1} = \arg\min_{x} \ f(x_t) + \langle x - x_t, \nabla f(x_t) \rangle + \frac{1}{2\eta_t}\|x-x_t\|^2.
\end{equation}
$$

The second formulation can be obtained from the first in a straightfoward manner. By rescaling the constraint, it is easy to see that we can get the usual descent $-\|\nabla f(x_t) \|_*^2$, which follows by the definition of the [dual norm](https://en.wikipedia.org/wiki/Dual_norm) $$\|\cdot\|_*^2 = \max_{\|v\| \leq 1} \langle v, \nabla f(x_t) \rangle$$, where we reparameterize $v = \frac{1}{\sqrt{2\eta_t}}(x-x_t)$. Squaring both sides of the constraint and adding a Lagrange multiplier, we get the second sub-problem.

Note the correspondence between both interpretations (namely, the neighborhood distance interpretation and the Hessian assumption interpretation). They are kind of the same in the following sense: *the Hessian we assume is the metric used in the neighborhood distance*. If we assume that the Hessian is some matrix $D_t$, then we can equivalently use the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) $\|x-x_t\|_{D_t} = \sqrt{\langle x-x_t, D_t(x-x_t) \rangle}$. Indeed, if we think about the distance constraint or the Hessian assumption for a moment, it seems obvious in hindsight that having all dimensions vary at the same rates is not always great idea. Consider a deep neural network, for example. Should we really use the same learning rate for the first layer as, say, the hundredth layer? Probably not. Do batch norm parameters vary at the same rate? As for the valley analogy (remember, you're blindfolded), descending a very thin valley can benefit from a very long stick in one direction, but would need a very short stick in the other direction. Hmmm... What to do?

We cannot improve much over the linear approximation as we have access to (unbiased estimates of) the gradient. However, the second approximation is more amenable to improvement especially if we have some *prior knowledge* about $\mathcal{X}$. The idea of using a better "metric" has been shown to be very effective in deep learning. Using the Mahalanobis distances described above, we can obtain improved metrics $D_t$ by minimizing the final regret with respect to them in hindsight. This, in fact, leads to the beautiful [AdaGrad](https://jmlr.org/papers/v12/duchi11a.html), the optimization algorithm on which the ubiquitous [Adam](https://arxiv.org/abs/1412.6980) is based.

In this post, I will talk about another interesting, and surprisingly different, approach based on a simple generalization of the expected risk minimization objective. Before that, we motivate the approach by walking through specialized versions of gradient descent that work better on structured space.


## Descending the simplex valley
Consider the ($d-1$)-dimensional probability simplex $\mathcal{X}=\Delta^{d-1} = \{x:\|x\|_1 = 1,x\in[0,1]^d\}$, where $\|\cdot\|$ is the L1 norm (the sum of the entries in absolute values). The vectors of this space can be thought of as "probability distributions" on $d$ items. You can define a mixture variable in your model that mixes between $d$ stuff, and you want to use gradient descent to train it. Can you use the scheme described above? The answer is no. Life is not always that simple. You just can't take the minimum over an unconstrained space and claim that it also is the minimum for the constrained space as well. I mean, it is very easy for the minimizer to violate the constraint $\|x\|_1 = 1$. So, what to do?

One can use projected gradient descent, which simply projects $x_{t+1}$ on $\mathcal{X}$, which is often an easy operation. A projection on the simplex [exists](https://optimization-online.org/2014/08/4498/) and is not exactly easy (i.e., simple). However, some other spaces with magnitude constraints do have simple projections which only consist of prunings. Anyways, we can project onto the simplex after gradient descent. Are we done? No. Life is not always that simple.

Euclidean distances do not make sense for probability distributions because of their constraints. So, what can we use to measure the difference between two probability distributions?
Some inspiration first. In machine learning, we often like to use negative log-probabilities as they are unconstrained from above (i.e., positive) and do not necessarily sum to 1. Can we take some sort of a difference between the negative logs instead as the distance instead? Yes, indeed, we can. This is exactly the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) (before taking expectation), which is the most popular measure of difference between two probability distribution in machine learning.

Does this suggest that we can use KL-divergences instead of Euclidean distances in the descent sub-problem? Yes, sort of! Let us first formalize the family of these distances and how to qualify to joing the family.

## Bregman divergence
Looking at the second formulation of the sub-problem (2), we can write it as the minimizer of some model of the objective $\arg\min_{x_t}\hat{f}(x_{t+1})$ such that

$$
\begin{equation}
    f(x_{t+1}) \approx \hat{f}(x_{t+1}) = f(x_t) + \langle x_{t+1} - x_t, \nabla f(x_t) \rangle + {\frac{1}{2\eta_t}\|x_{t+1}-x_t\|^2}.
\end{equation}
$$

Here, we modeled the objective as a second-order approximation of $f$ with some assumptions on the second (and higher)-order terms. In general, for some strictly convex and differentiable function $\psi$, the second and higher-order terms behave as some sort of "divergence"

$$
\begin{equation}
    \psi(x_{t+1}) - \psi(x_t) - \langle x_{t+1} - x_t, \nabla \psi(x_t) \rangle =: B_{\psi}(x_{t+1},x_t),
\end{equation}
$$

which is exactly the definition of the [Bregman divergence](https://en.wikipedia.org/wiki/Bregman_divergence) associated with $\psi$. Indeed, $\psi$ being strictly convex implies $B_{\psi}(x, x') > 0, \forall x \neq x'$ and $B_{\psi}(x, x)=0, \forall x$. However, note that bregman divergences are NOT symmetric in general, i.e., we can have $B_{\psi}(x, x') \neq B_{\psi}(x', x)$ for some $x,x'$.

We can now impose our assumptions on the divergence terms directly on $\psi$. For example, it is easy to see that, using $$\psi = \|\cdot\|_{D}^2$$ for some positive-definite $D$, we have $B_{\psi}(x, x') = \|x-x'\|_D^2$, which also recovers the Euclidean distance when $D$ is identity. Other strictly convex functions give other bregman divergences, though there aren't many! And I find this to be very interesting. Here is a (non-exhaustive) table from [Nock & Nielsen (2005)](https://franknielsen.github.io/SlidesVideo/BCBregmanBalls-ECML-2005.pdf) that shows the bregman divergences associated with different strictly convex functions and their functional averages (explained below) on the last column:

![Bregman divergences and their associated functional averages](/assets/img/bregman_nock_nielsen_2005.png)

Other divergences includes the Hellinger distance and the linear-exponential (linex) loss (see Table 1 in [Ackermann and Blömer (2009)](https://arxiv.org/abs/1508.05243), e.g.) To clarify the notation, note that $c$ is a vector and $s$ is an "arbitrary" vector in the second and third column, whereas in the fourth column, $s_i$ are given vectors $1 \leq i \leq m$ (i.e., data).
The *functional average* in the fourth column could be seen as the minimizer of the "smallest enclosing" Bregman divergence with respect to the first argument. Ignoring the "smallest enclosing" constraint and setting $\alpha_i=1/m$, the $c$ in the last column is simply $\arg\min_{c} \frac{1}{m} \sum_{i=1}^m D_F(c,s_i)$. The $\alpha_i$ are just Lagrange multipliers that come from the constrained problem: $\arg\min_{r,c} \ r$ such that $D_F(c,s_i) \leq r$ for $1 \leq i \leq m$ (in the spirit of support vector machines). Namely, since $\frac{\partial}{\partial c} D_{F}(c,s_i) = \nabla F(c) - \nabla F(s_i)$, the functional average is $c^\ast = (\nabla F)^{-1}(\frac{1}{m} \sum_{i=1}^m \nabla F(s_i))$. Indeed, this is the arithmetic average when $F$ is the L2 norm as shown in the table There is also an interesting parallel in Bayes decision theory. When the risk of a classifier $\hat{y}$ is $$R(\hat{y}) = \frac{1}{2} \E_{x,y} \|\hat{y}(x) - y\|^2$$, then the Bayes optimal classifier is $$\hat{y}(x) = \E_{y|x}[y]$$, which is like the arithmetic average. For L1 distance risk, we get the median instead of the average, but this is not necessarily the same for Bregman divergences! Remember that Bregman divergences are associated with *differentiable* and strictly convex functions (but see the last row in the table and take $p \to 1$).

Anyways, for the probability simplex case $\Delta^d$, we can generate the appropriate divergence using the negative entropy $\psi(x) = \sum_{i=1}^d x_i \log x_i$ so that $B_{\psi}(x,x') = \sum_{i=1}^d x_i \log \frac{x_i}{x_i'}$, which is the KL-divergence. Note that the additive terms cancel out because $x,x' \in \Delta^d$, where the general (unnormalized) case yields the Information divergence instead.

Using bregman divergences, we can generalize the descent sub-problem as follows

$$
\begin{equation}
    x_{t+1} = \arg\min_x \ f(x_t) + \langle x - x_t, \nabla f(x_t) \rangle + \frac{1}{\eta_t} B_{\psi}(x,x_t).
\end{equation}
$$


## Mirror descent

The sub-problem we have now is precisely [mirror descent](https://en.wikipedia.org/wiki/Mirror_descent). First, note that $\frac{\partial}{\partial x} B_{\psi}(x,x_t) = \nabla \psi(x) - \nabla \psi(x_t)$. Then,

$$
\begin{align}
    \frac{\partial}{\partial x} \left(\langle x, \nabla f(x_t) \rangle + \frac{1}{\eta_t} B_{\psi}(x,x_t) \right) &= 0 \\
    \implies \nabla \psi(x^{\text{opt}}) &= \nabla \psi(x_t) - \eta_t \nabla f(x_t),
\end{align}
$$

which is a minimizer since $\psi$ is strictly convex (i.e., $\nabla^2 \psi(x) \succ 0$). Note this is just gradient descent when $\psi = \frac{1}{2}\|\cdot\|_2^2$. Further, when $\psi = \frac{1}{2}\|\cdot\|_D^2$, we get preconditioned gradient descent, which includes AdaGrad and Newton step.

This is called mirror descent because you are descending in the "mirror" world, aka the dual space. In other words, you go to the dual space, take a regular (Euclidean) gradient step, and then come back. Namely,

$$
\begin{equation}
    x_{t+1} = (\nabla \psi)^{-1}(\nabla \psi(x_t) - \eta_t \nabla f(x_t)),
\end{equation}
$$

where the inverse is element-wise. This is like a change of coordinates that makes the Euclidean metric "more natural". For Euclidean space, the rate of change of the norm is proportional to the variable itself, so no change of coordinates is required. On the other hand, for spaces with general norms of quadratic form $\|x\|_D^2=x^TDx$, we can linearly transform the coordinates $x'=D^{1/2}x$ so that the rate of change of the norm in $x'$ space is proportional to $x'$. This intuition will help us understand the Bayesian learning rule later.

We can explicitly solve the equation for $x$ by figuring out $(\nabla \psi)^{-1}$.
In fact, $(\nabla \psi)^{-1} = \nabla \psi^\ast$, where $\psi^\ast$ is the convex conjugate of $\psi$. This is not clear at all, so let us see why it holds.

Recall the definition of the convex conjugate $\psi^\ast : \mathcal{X}^\ast \to \mathbb{R}$, where $\mathcal{X}^\ast$ is the dual space of $\mathcal{X}$

$$
\begin{equation}
    \psi^\ast(g) := \sup_{x} \langle g, x \rangle - \psi(x).
\end{equation}
$$

Let us take the supremem for each $\psi^\ast(g)$

$$
\begin{equation}
    \frac{\partial}{\partial x}\left(\langle g, x \rangle - \psi(x)\right) = 0
    \quad\implies\quad
    g = \nabla \psi(x^{\text{opt}}).
\end{equation}
$$

$x^{\text{opt}}$ is a maximizer since $-\psi(x)$ is strictly concave. Thus, for $x^{\text{opt}}$, the definition holds with equality $\psi^\ast(g) = \langle g, x^{\text{opt}} \rangle - \psi(x^{\text{opt}})$.
By taking the gradient with respect to $g$, we get $\nabla \psi^\ast(g) = x^{\text{opt}}$. Thus,

$$
\begin{equation}
    g = \nabla\psi(\nabla\psi^\ast(g))
    \quad \text{and} \quad
    x^{\text{opt}} = \nabla\psi^\ast(\nabla\psi(x^{\text{opt}})),
\end{equation}
$$

which implies that $(\nabla\psi)^{-1} = \nabla\psi^\ast$.

Well, I am not claiming that this proof is rigorous, but it does the job. I mean, the maximizer $x^{\text{opt}}$ kind of depends on $g$ (it's right there in the definition of the convex conjugate), so why did we not differentiate through it? I don't know why, but another intuitive way to look at it is through the subgradient (which becomes a gradient here because $x^{\text{opt}}$ is unique due to the strict convexity of $\psi$). Recall the definition of a subgradient $g$ of $\psi$ at $x$

$$
\begin{equation}
    g \in \partial\psi(x) \iff \psi(x') \geq \psi(x) + \langle g, x'-x \rangle, \ \forall x' \in \mathcal{X}.
\end{equation}
$$

Let $x \in \mathcal{X}$ and $g \in \mathcal{X}^\ast$. Recall Fenchel-Young inequality (which directly follows from the definition of the convex conjugate)

$$
\begin{equation}
    \langle x, g \rangle \leq \psi(x) + \psi^\ast(g),
\end{equation}
$$

which holds with equality when $g \in \partial\psi(x)$ (you can prove it as an exercise). Using the biconjugate $\psi^{**}$, we also have

$$
\begin{equation}
    \langle x, g \rangle \leq \psi^{**}(x) + \psi^\ast(g),
\end{equation}
$$

which similarly holds with equality when $x \in \partial\psi^\ast(g)$. Thus, since $\psi = \psi^{**}$ for lower semi-continuous $\psi$, we get

$$
\begin{equation}
    g \in \partial\psi(x) \iff x \in \partial\psi^\ast(g).
\end{equation}
$$

This is an even more general form of $(\nabla\psi)^{-1} = \nabla\psi^\ast$. For a rigorous treatment of the above, see Theorem 23.5 in [Convex Analysis (Rockafellar, 1970)](https://convexoptimization.com/TOOLS/AnalyRock.pdf).

Anyways, moving on.

<br>

---

<br>

# Descending a valley, but you're blindfolded and drunk
Sometimes you can't observe your steps very well or can't maintain the exact trajectory that you wanted with full accuracy. In other words, you want to be robust to slight "errors" in your calculated steps. The previous analysis works when we have access to gradients or unbiased estimates thereof (i.e., stochastic gradients). However, it assumes that we know the iterates exactly, which makes sense.
So this problem is simply written as

$$
\begin{equation}
    \arg\min_{x} f(x) + \Gamma(x),
\end{equation}
$$

for some regularizer $\Gamma$, e.g., $\Gamma(x) = \frac{\lambda}{2}\|x\|_2^2$.

We can change it slightly to allow for "errors" in $x$. We can either minimize with respect to the worst case scenario or the expected scenario. Let $\varepsilon$ be some random variable, e.g., $\varepsilon \sim \mathcal{N}(0, \sigma^2 I_d)$ the centered normal distribution with variance $\sigma^2$. As for the worst case scenario, since we know that the normal distribution is unbounded, we might instead want to make some boundedness assumption on $\varepsilon$, e.g. $\|\varepsilon\|_2 \leq \rho$.

We can now define the two following problems

$$
\begin{align}
    &\arg\min_{x}\ \max_{\|\varepsilon\|_2 \leq \rho}\ f(x + \varepsilon) + \frac{\lambda}{2}\|x\|_2^2 \tag{SAM} \\
    &\arg\min_{x}\ \E_{\varepsilon \sim \mathcal{N}(0, \sigma^2 I_d)}\ [f(x+\varepsilon)] + \frac{\lambda}{2}\|x\|_2^2 \tag{Bayes}.
\end{align}
$$

The first objective $\text{(SAM)}$ is exactly the ones used in [sharpness-aware minimization](https://arxiv.org/abs/2010.01412) (hence the acronym). The authors used a linear approximation in $\varepsilon$, which results in a closed form max $f\left(x+\rho\frac{\nabla f(x_t)}{\|\nabla f(x_t)\|_2}\right)$. The gradient is then taken at this perturbed point *with respect to the unperturbed point*, i.e., $\frac{\nabla f(x_t)}{\|\nabla f(x_t)\|_2}$ is fixed with a stop-gradient operator as if it was an adversarial noise.

Ok, what we are really interested in is the second objective. It looks very similar to the first one, right? The name $\text{(Bayes)}$ mostly comes from the use of expectation. It also comes from the fact that it can be interpreted as finding the [maximum a posteriori](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) estimator $\theta$ that has a prior $\mathcal{N}(0, \frac{1}{\lambda} I_d)$ and a posterior $\mathcal{N}(x, \sigma^2 I_d)$.

In fact, it has been shown recently that $\text{(Bayes)}$ is very much related to $\text{(SAM)}$. The answer is in the title of the paper that explains it: ["SAM as an Optimal Relaxation of Bayes"](https://arxiv.org/abs/2210.01620). Namely, $\text{(SAM)}$ is an optimal relaxation of $\text{(Bayes)}$, where the relaxation is based on the convex biconjugate $f^{\ast\ast}$ and the word "optimal" comes from the fact that $f^{\ast\ast}$ is the optimal convex lower bound of $f$.

## An information-theoretic objective

The $\text{(Bayes)}$ objective is more general in the sense that we can think in terms of posterior distributions of $x \sim q_t$ rather than hard iterates $x_t$. Indeed, $(\text{Bayes})$ can be written in terms of distributions using a variational formulation called the *optimal information-processing rule* $\text{(OIPR)}$, which is due to [Zellner (1988, eq. 2.8)](https://ageconsearch.umn.edu/record/296078?ln=en&v=pdf)

$$
\begin{equation}
    \arg\min_q\ \E_{x \sim q} [f(x)] + D_{\text{KL}}(q(x)\, \|\, p(x) ), \tag{OIPR}
\end{equation}
$$

where p is the prior of $x$, $q$ is the posterior given data $\xi$, and $D_{\text{KL}}$ is the KL-divergence (acting as a regularizer of $q$ with respect to $p$). Assuming a faithful **negative log-likelihood (NLL) loss** $f(x) \propto -\log p(\xi \mid x)$, the minimizer of  $\text{(OIPR)}$ recovers the true posterior $p(x \mid \xi)$, which corresponds to [Bayesian inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) as a special case (see below).

Perhaps practitioners of Bayesian methods might be slightly confused by the notation and KL-divergence, so let us clarify things a bit. First, we are sticking with the optimization notation where $f$ is the loss, $x$ is the parameter (acting as the "latent" variable), and $\xi$ is the dataset (usually written in the literature as pairs of $x_i$ and $y_i$). Another source of confusion might be $D_{\text{KL}}(q(x)\, \|\, p(x) )$ since $q(x)$ should actually approach $p(x \mid \xi)$ as it gets better. The corresponding [evidence lower bound (ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound) should look something like this

$$
\begin{align}
    \mathcal{L}(q)
    &:= \E_{x \sim q}\left[ \log \frac{p(x,\xi)}{q(x)} \right] \\
    &= \underbrace{\log p(\xi)}_{\text{evidence}} - \underbrace{D_{\text{KL}}(q(x)\, \|\, p(x|\xi) )}_{\geq 0}
    \leq \log p(\xi).
\end{align}
$$

Maximizing the ELBO implies a better lower bound for the evidence, but how is this a special case of $\text{(OIPR)}$? It is straightforward, actually. Just plug in the negative log-likelihood loss
    
$$
\begin{align}
    \text{(OIPR)'s obj.}
    &= \E_{x \sim q} \left[\log \frac{q(x)}{p(\xi|x)p(x)}\right] + \text{const}
    \\&= \E_{x \sim q} \left[\log \frac{q(x)}{p(x|\xi)p(\xi)}\right]+ \text{const}
    \\&= -\text{(ELBO)} + \text{const},
\end{align}
$$

so minimizing $(\text{OIPR})$ given $f(x) \propto -\log p(\xi \mid x)$ implies maximizing the ELBO, as shown in [(Khan & Rue, 2023, p. 28)](https://arxiv.org/abs/2107.04562). (There is actually a small typo there where they flipped the sign of the evidence.)

### Beyond "point" gradients

We now come to a question that represents the crux of this blog post. **Can we solve $\text{(OIPR)}$ directly with gradient descent**? What does it mean to take a gradient descent step on $q$? Does $q_{t+1} = q_t - \eta_t \nabla_q \left(\E_{x \sim q} [f(x)] + D_{\text{KL}}(q(x)\, \|\, p(x) ) \right)$ make sense? Yes, kind of! Not only does it make sense, we should sometimes think about gradient descent from this perspective. Let us see why next.

## Descending like a Bayesian

In general, posteriors are positive functions that integrate to 1. A perturbed posterior $\tilde{q}$ can be controlled in terms of norms, e.g. $\|\tilde{q}-q\|_1 < \epsilon$. But in general, it is intractable to "describe" such functions for every possible sample $x \in \mathcal{X}$, let alone perturb them (i.e., as in lookup tables). Thus, what we we usually with probability distributions is to *parameterize* them, add some constraints and assumptions, if any, and operate on the parameters instead. The constraints and assumptions describe the distribution family and the parameter vector act as a member of the family. Quite often, this parameter space is not Euclidean, i.e., the parameters entries might not vary linearly with respect to each other. For example, we might know some *natural* parameters that can fully characterize a member of the posterior family, such as the mean and covariance for normal distribution, each of which affects the sampled output in a completely different manner (one affects position, the other affects deviation). We will see that optimizing $\text{(OIPR)}$ with [exponential family](https://en.wikipedia.org/wiki/Exponential_family) posteriors corresponds to preconditioned gradient descent next (Note that the exponential family of distributions is NOT the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution), which is an unfortunate nomenclature. In fact, the exponential family includes the exponential and normal distributions as special cases).

It is worth mentioning that working on parameteric distributions is certainly not the only tractable way forward, at least analytically. For example, analysis on the Wasserstein space (i.e., distributions with $\|q\|_2 < \infty$) is possible and can have some niche practical implications [(Bonet et al. , 2024)](https://arxiv.org/abs/2406.08938).

## Minimizing perturbations around the mean
An interesting thing happens when you minimize the second-order taylor expansion of the functional $F(q) := \E_{x \sim q} [f(x)]$ around the mean $\mu := \E_{x \sim q} [x]$ (we will ignore the KL divergence term for now)

$$
\begin{align}
    F(q) &\approx f(\mu) + \E_{x \sim q} (x - \mu)^T\nabla f(\mu) + {\small \frac{1}{2}}\E_{x \sim q} (x - \mu)^T\nabla^2 f(\mu) (x - \mu) \\
    &= f(\mu) + {\small \frac{1}{2}} \E_{x \sim q} \|x - \mu\|^2_{\nabla^2 f(\mu)}.
\end{align}
$$

Let $\Sigma := \E_{x \sim q} [(x-\mu)(x-\mu)^T]$ be the covariance of $x$. Then,
    
$$
\begin{align}
    F(q) &\approx f(\mu) + {\small \frac{1}{2}}\E_{x \sim q} \|x - \mu\|^2_{\nabla^2 f(\mu)} \\
    &= f(\mu) + {\small \frac{1}{2}}\E_{x \sim q} \text{Tr}((x - \mu)(x - \mu)^T \nabla^2 f(\mu)) \\
    &= f(\mu) + {\small \frac{1}{2}}\text{Tr}(\Sigma \nabla^2 f(\mu)) \\
    &= f(\mu) + {\small \frac{1}{2}}\| \nabla^2 f(\mu) \|^2_{\Sigma^{1/2}}.
\end{align}
$$

In other words, approximating the objective $\arg\min_q F(q)$ given the mean and covariance gives us

$$
\begin{equation}
    \arg\min_{q\ | \ \mu,\ \Sigma} \E_{x \sim q} [f(x)] \approx \arg\min_{\mu, \ \Sigma} f(\mu) + \| \nabla^2 f(\mu) \|^2_{\Sigma^{1/2}}.
\end{equation}
$$

Thus, we cast the problem of minimizing the Bayesian objective with respect to posteriors $q$ to a more tractable problem in terms of the mean and covariance of $q$ only, which is accurate up to second-order terms. Interestingly, for $\Sigma=I$, the Bayesian objective *implicitly flattens/smoothes the minimizer* via a Hessian sharpness penalty (i.e., minimizing the maximum eigenvalue of $\nabla^2 f(\mu)$). In some sense, $\Sigma$ can be seen as a preconditioner (more details on this later), so it is true that optimizing the objective with respect to $\Sigma$ can yield lower objective values, but *perhaps* this could happen at the expense of a sharper **Euclidean** landscape.

## The Bayesian Learning Rule (BLR)

We now consider the more general parameteric case. Precisely speaking, let the parameter of posterior $q$ be $\theta \in \mathbb{R}^D$ for some $D$, so the posterior can be denoted as $q_\theta(x)$.

### The exponential family of distributions
The first component of the [Bayesian Learning Rule](https://arxiv.org/abs/2107.04562) is the assumption that $q_\theta(x)$ belongs to the exponential family, a member of which shall be denoted as an EF distribution hereafter (Note: I'm using $\theta$ to denote the natural parameter and $x$ as a sampled model, which is different from the BLR paper, so I'm sorry about the confusing notation, but I like it this way more)

$$
\begin{equation}
    q_\theta (x) = h(x) \exp \left( \langle \theta, T(x) \rangle - A(\theta) \right). \tag{EF}
\end{equation}
$$

This family of distributions has a simple intuition behind it. $\theta$ is called the natural parameter (e.g., $\theta = [\mu; \sigma^2]$ for normal distributions). $A(\theta)$ is just the log-partition function $\log \int h(x) e ^{ \langle \theta, T(x) \rangle} dx$, which normalizes the probability $q_\theta (x)$.
The function $h(x)$ is the base measure, i.e., $q_{0}(x) \propto h(x)$. In a measure theoretic formulation, we can write $dq_{\theta}(x) = e ^{ \langle \theta, T(x) \rangle - A(\theta)} dh(x)$. As we are interested in maximizing $\log q_\theta (x)$ over $\theta$, we can simply ignore $h(x)$. Finally, $T(x)$ is the sufficient statistic, and we can see that the log-probability is proportional to $\langle \theta, T(x) \rangle$, so we only need $T(x)$ from $x$ in order to know $q_{\theta}(x)$.

The exponential family can be written in many forms, but the one above is the one that "makes sense" when we want to maximize $\log q_{\theta}(x)$.
In other words, we want to find $\theta$ such that $T(x) = \nabla A(\theta)$.
Since $A(\theta)$ is strongly convex and differentiable, we have a maximum $\theta^\ast = (\nabla A)^{-1}(T(x))$.
But on the other hand, $\sup_{\theta} \langle \theta, T(x) \rangle - A(\theta)$ is precisely the definition of $A^\ast(T(x))$.
Oh wait, we have seen something like this before! Looking back at the mirror descent section, we can see that $\theta^\ast = (\nabla A)^{-1}(T(x)) = \nabla A^\ast(T(x))$. This is one of the perks of the exponential family. More on this later.


### The BLR update
The second component of the Bayesian Learning Rule is the update

$$
\begin{equation}
    \theta_{t+1} = \theta_{t} - \eta_t \tilde{\nabla}_{ {\color{brown}\theta_t}} f_{\text{OIPR}}(q_{ {\color{brown}\theta_t}}), \tag{OIPR-step}
\end{equation}
$$

where $f_{\text{OIPR}}(q) = \E_{x \sim q} [f(x)] + D_{\text{KL}}(q(x)\, \|\, p(x) )$ and $\tilde{\nabla}$ is the [natural gradient](https://yorkerlin.github.io/posts/2021/10/Geomopt02/).

By assuming a regularizer $\Gamma(x) = - \log p(x)$ and
noting that $D_{\text{KL}}(q(x)\, \|\, p(x) ) = \E_{x \sim q} [\Gamma(x)] - H(q)$, where $H(q) = -\E_{x \sim q}[\log q(x)]$ is the entropy,
the BLR paper writes $f_{\text{OIPR}}(q)$ in terms of the loss $f$, the regularizer $\Gamma$, and the entropy $H$ as

$$
\begin{equation}
    \theta_{t+1} = \theta_{t} - \eta_t \tilde{\nabla}_{ {\color{brown}\theta_t}} (\E_{x \sim q_{ {\color{brown}\theta_t}} } [f(x) + \Gamma(x)] - H(q_{ {\color{brown}\theta_t}})). \tag{BLR-step}
\end{equation}
$$


Recall that for a NLL loss, i.e., $f(x) \propto - \log p(\xi \mid x)$, optimizing $\text{(OIPR)}$ becomes equivalent to maximizing the ELBO, and the optimal solution is $q^\ast = p(x \mid \xi)$.


### Natural gradient and duality
One of the elegant consequences of combining these two components is due to the presence of the natural gradient $\tilde{\nabla}$. The natural gradient uses the inverse Fisher information matrix $F(\theta)$ as a preconditioner for the gradient

$$
\begin{equation}
    \tilde{\nabla}_{\theta} \E_{x \sim q_{\theta}}(\cdot)
    = F(\theta)^{-1} \nabla_{\theta} \E_{x \sim q_{\theta}} (\cdot).
\end{equation}
$$

What is the [fisher information matrix](https://en.wikipedia.org/wiki/Fisher_information)? It is simply the covariance matrix of the score $\nabla_\theta \log q_\theta(x)$

$$
\begin{equation}
    F(\theta)
    = \E_{x \sim q_{\theta_t}} [\nabla_\theta \log q_\theta(x) \nabla_\theta \log q_\theta(x)^T]
    = \E_{x \sim q_{\theta_t}} [-\nabla_\theta^2 \log q_\theta(x)]
    , \tag{Fisher}
\end{equation}
$$

which is a covariance since $\E_{x \sim q_{\theta_t}} [\nabla_\theta \log q_\theta(x)] = 0$ (as long as the regularity conditions for $\int \nabla_\theta = \nabla_\theta \int$ hold).
The second identity can be proved by solving it directly (this time, assuming $\int \nabla_\theta^2 = \nabla_\theta^2 \int$).
This directly implies, from the expression of $q_{\theta}(x)$, that

$$
\begin{equation}
    F(\theta) = \nabla_{\theta}^2 A(\theta). \tag{EF-Fisher}
\end{equation}
$$

Interesting. Does BLR update recovers Newton step when $A = f_\text{OIPR}$? We will see later.

Using the fact that the expectation of the score is 0,
we immediately see, again from the expression of $q_{\theta}(x)$, that

$$
\begin{equation}
    \nabla A(\theta) = \E_{x \sim q_{\theta}}[T(x)]
    := \nu(\theta)
    \tag{Expect. param.}
    ,
\end{equation}
$$

which is the definition of the *expectation parameter* $\nu$. It is very similar to the condition of the maximizer $\theta$ for $\log q_\theta (x)$, but here we have the sufficient statistic *in expectation*. Note that the equality above holds *for all $\theta$* due to the fact that the expectation of the score is 0 for all $\theta$.

The duality between $\theta$ and $\nu$ is now clear. The definition of the expectation parameter and the discussion in the mirror descent section already hint at this duality.
Indeed, we have

$$
\begin{equation}
    \nu = \nabla A(\theta)
    , \quad\quad
    \theta = \nabla A^\ast(\nu)
    . \tag{BLR duality}
\end{equation}
$$

We dropped the dependence of $\nu$ on $\theta$ here on purpose because $\theta$ depends on $\nu$, too!
This is exactly the same story when we said $\nabla \psi^\ast = (\nabla \psi)^{-1}$. The parameters $\theta$ and $\nu$ exist in dual worlds and are coupled via $A$ as much as $x$ and $g$ are coupled via $\psi$ in mirror descent.

Given this duality, the following elegant formula of the natural gradient can be derived with some derivative shenanigans

$$
\begin{equation}
    \tilde{\nabla}_{\theta} \E_{x \sim q_{\theta}}(\cdot)
    = \nabla_{\nu} \E_{x \sim q_{\theta}} (\cdot)
    .
\end{equation}
$$

This follows from the fact that $\nabla_\theta \nu(\theta) = F(\theta)$. Focusing solely on the derivative operator, we can write

$$
\begin{equation}
    \tilde{\nabla}_{\theta}
    = F(\theta)^{-1} \nabla_{\theta}
    = \left( \frac{\partial \nu(\theta)}{\partial \theta} \right)^{-1} \frac{\partial}{\partial \theta}
    = \frac{\partial}{\partial \nu(\theta)}
    = \nabla_{\nu(\theta)}
    .
\end{equation}
$$

This derivation may not be rigorous, but it gives an intuition as to why this unusual identity holds. Informally, we can say that the natural gradient in $\theta$ space is equal to the Euclidean gradient in $\nu$ space.

### Why natural gradients?
Ok, so, *is the choice of the natural gradient just a matter of convenience*? Not really. At least not when $q$ is from the exponential family of distributions.

One important property of natural gradient descent is that it is invariant to reparameterizations of $\theta$, so it depends directly on $q_\theta$. This is one of the main ideas behind [information geometry](https://en.wikipedia.org/wiki/Information_geometry). For example, it doesn't matter whether you use the standard deviation or the variance as the parameter of your distribution.

Another interesting reason stems from the gradient of the entropy in the $\text{(BLR-step)}$. Assuming that $h(x)$ is constant, the natural gradient of the negative entropy $-H(q)$ is simply $\theta$, whereas the Euclidean gradient is $\nabla^2 A(\theta) \theta$ (see the appendix for details). Let $F(q) := \E_{x \sim q } [f(x) + \Gamma(x)]$ and consider a fixed point $\theta^\text{opt}$ of the $\text{(BLR-step)}$ so that

$$
\begin{equation}
    0 = \tilde{\nabla}_{\theta} (F(q_{\theta^\text{opt}}) - H(q_{\theta^\text{opt}})).
\end{equation}
$$

This implies the following identity of the fixed point

$$
\begin{equation}
    \theta^\text{opt} = -\tilde{\nabla}_{\theta} F(q_{\theta^\text{opt}}).
\end{equation}
$$

Even if we use the Euclidean gradient in the $\text{(BLR-step)}$, we would have

$$
\begin{equation}
    \nabla^2 A(\theta^\text{opt}) \theta^\text{opt} = -\nabla_{\theta} F(q_{\theta^\text{opt}}),
\end{equation}
$$

which immediately implies the previous identity since $F(\theta) = \nabla^2 A(\theta^\ast)$. This suggests that the natural gradient is inherently in every solution of the BLR problem.

There is a simple interpretation behind this. Consider a regularized problem $f(x) + \frac{1}{2}\|x\|_D^2$ for some $D \succ 0$. The optimal $x^\text{opt}$ is characterized by the identity $Dx^\text{opt} = -\nabla f(x^\text{opt})$, so we can similarly argue that the preconditioned gradient $D^{-1} \nabla f(x)$ is in every solution. However, this is particularly because $D$ is the underlying metric of the search space that induces this regularization (i.e., $x^\text{opt}$ is the minimizer s.t. $\frac{1}{2}\|x\|_D^2 \leq 1$). For the BLR problem, the metric is $F(\theta) = \nabla^2 A(\theta)$.

It is important to note that the Euclidean gradient of the entropy of $q_{\theta}$ is an invertible linear transformation of $\theta$. Since the entropy is strictly concave in $q$, we can write the inverse of its gradient $(\nabla H)^{-1}$ as the gradient of its conjugate $\nabla H^\ast$ (see the mirror descent section above). Thus, we can obtain the following generalized expression

$$
\begin{equation}
    \theta^\text{opt} = \nabla_{\theta^\ast} H^\ast(-\nabla_\theta F(q_{\theta^\text{opt}})),
\end{equation}
$$

where $\theta^\ast$ denotes the dual space of the parameters. Indeed, this can intuitively understood as mirror descent on distributions with $H$ as the potential (which induces the metric via Bregman divergence). This gives an intuition on the correspondence between potentials and entropies (resp., divergences and metrics). We will see next how we can make this correspondence more concrete.

### Deriving BLR from steepest descent
The BLR update is intimately related to mirror descent, and mirror descent is just gradient descent in the dual space, or in some sense, the space where the coordinates are mapped such that the metric is Euclidean.

Natural gradient descent algorithms, similar to mirror descent, obtains the descent step from minimizing the following sub-problem

$$
\begin{equation}
    \theta_{t+1} \gets \arg\min_{ {\color{brown} \theta}} \langle \nabla_\theta f(q_{\theta_t}), {\color{brown} \theta} \rangle + \frac{1}{\eta_t} D_\text{KL} (q_{ {\color{brown} \theta}} \| q_{\theta_t}).
\end{equation}
$$

The minimization of the KL-divergence can be done easily with some second-order approximation magic (you can find a derivation [here](https://yorkerlin.github.io/posts/2021/11/Geomopt04/), for example)

$$
\begin{equation}
    D_\text{KL} (q_{ {\color{brown} \theta}} \| q_{\theta_t})
    \approx ({\color{brown} \theta} - \theta_t)^T F(\theta_t) ({\color{brown} \theta} - \theta_t).
\end{equation}
$$

Using this approximation in above the sub-problem, we obtain natural gradient descent $\theta_{t+1} \gets \theta_t - \eta_t F(\theta_t)^{-1}\nabla_\theta f(q_{\theta_t})$.

We can derive the same exact update with mirror descent if we assume that $q_\theta$ is a distribution from the exponential family, i.e., $q_\theta(x) = h(x) \exp \left( \langle \theta, T(x) \rangle - A(\theta)\right)$.
In fact, we will see that mirror descent in the dual space is exactly natural gradient descent because the second-order approximation above is exact for EF distributions.

Recalling the duality $\nu = \nabla A(\theta)$ and $\theta = \nabla A^\ast(\nu)$, the KL-divergence can conveniently become

$$
\begin{align}
    D_\text{KL} (q_{ {\color{brown} \theta}} \| q_{\theta_t})
    &= \E_{x \sim q_{ {\color{brown} \theta}}} [\log q_{ {\color{brown} \theta}}(x)]
    - \E_{x \sim q_{ {\color{brown} \theta}}} [\log q_{\theta_t}(x)]
    \\ &= 
    \langle {\color{brown} \theta}, \nu({\color{brown} \theta}) \rangle - A({\color{brown} \theta})
    - \langle \theta_t, \nu({\color{brown} \theta}) \rangle + A(\theta_t)
    \\ &= 
    A(\theta_t) - A({\color{brown} \theta}) - \langle \theta_t - {\color{brown} \theta}, \nabla A({\color{brown} \theta}) \rangle
    \\ &= 
    B_{A} (\theta_t, {\color{brown} \theta}),
\end{align}
$$

which is the Bregman divergence corresponding to $A$.
The divergence term is also equal to the reverse divergence in the dual space,
i.e., $B_{A} (\theta_t, {\color{brown} \theta}) = B_{A^\ast} (\nu({\color{brown} \theta}) , \nu(\theta_t))$. The derivation is kind of straightforward, so I have put it in the Appendix.

This equality implies that we can write the natural gradient descent sub-problem in terms of dual descent with respect to $\nu({\color{brown} \theta})$ given a reparameterized loss $\tilde{f}(\nu(\theta_t)) = f(q_{\theta_t})$ as follows

$$
\begin{align}
    \theta_{t+1} &\gets \arg\min_{ {\color{brown} \theta}} \langle \nabla_\theta f(q_{\theta_t}), {\color{brown} \theta} \rangle + \frac{1}{\eta_t} D_\text{KL} (q_{ {\color{brown} \theta}} \| q_{\theta_t})
    \\
    \iff \nu_{t+1} &\gets \arg\min_{ {\color{brown}\nu}} \langle \nabla \tilde{f}(\nu_t), {\color{brown}\nu} \rangle + \frac{1}{\eta_t} B_{A^\ast} ({\color{brown} \nu } , \nu_t)
    .
\end{align}
$$

Indeed, $\nabla_{ {\color{brown} \nu }} B_{A^\ast} ({\color{brown} \nu } , \nu_t) = {\color{brown} \nu } - \nu_t$ and $\nabla_\nu \tilde{f} = F^{-1} \nabla_\theta f$ (see the natural gradient and duality section above).


## BLR updates in the wild
Ok, so the most important question is... can we get something practical out of BLR?
Well, I wouldn't have written this blog post if it weren't for the versatility of BLR in deriving practical algorithms. The keen eye will notice that preconditioned gradient descent and Newton's method follow from BLR. This already covers a large number of algorithms, including AdaGrad, for example.

### Sufficient statistic
A very important concept that we introduced without much explanation is the [sufficient statistic](https://en.wikipedia.org/wiki/Sufficient_statistic). (Skip this if you already know enough about it.)

We implicitly defined the sufficient statistic by its presence in the expression of EF distributions $q_\theta(x) = h(x) \exp (\langle \theta, T(x) \rangle - A(\theta))$. Given the distribution parameter $\theta$, we can see that the distribution over $x$ is completely determined by its statistic $T(x)$ (up to normalization). In other words, knowing information $T(x)$ from sample $x$ suffices to estimate $\theta$, and knowing more than $T(x)$ is not necessary. (Note this does not mean that knowing $T(x)$ is necessary. That would be called the *minimal* sufficient $T(x)$.)

Sufficient statistics go beyond EF distributions, but in a very straightforward way. The Fisher–Neyman factorization theorem says that, in general, we have $q_\theta(x) = h(x) g_\theta(T(x))$ for non-negative $h$ and $g_\theta$. The EF case is when $g_\theta$ is a linear function. Furthermore, sufficient statistics can be transformed by bijections and still maintain their sufficiency. This is simply because we can always invert such transformations.

Let us see some examples. A sufficient statistic for normal distirbutions is simply $T(x) = [\hat{\mu}(x); \hat{\sigma}(x)^2]$, where $\hat{\mu}(x) = \frac{1}{n} \sum_i x_i$ is the sample mean and $\hat{\sigma}(x)^2 = \frac{1}{n-1} \sum_i (x_i - \hat{\mu}(x))^2$ the sample variance. Removing the normalization factors still maintain the sufficiency because all the information is "already there" but we just need to rescale it to correct for bias. In fact, $T(x)=\sum_i x_i$ is a sufficient statistic for both Poisson distributions and exponential distributions. Indeed, observe that both of them are EF distributions, as is the normal distribution, so it should be clear why the scale does not remove the sufficiency. As another intuitive example, a sufficient statistic for estimating the uniform distribution $\mathcal{U}(0, \theta)$ from samples $x = (x_1, \cdots, x_n)$ is $T(x) = \max_{1 \leq i \leq n} x_i$.

### Delta method
One common trick to derive updates is the *delta method*. We have already used a better version of it in the "Minimizing perturbations around the mean". The delta method is simply a first-order approximation around the mean, so what you will get is just the function itself evaluated at the mean. Namely, for $\mu := \E_{x \sim q_\theta}[x]$, we get

$$
\begin{equation}
    \E_{x \sim q_\theta}[f(x)]
    \approx
    \E_{x \sim q_\theta}[f(\mu) + \langle \nabla f(\mu), x - \mu \rangle]
    = f(\mu).
    \tag{Delta-0}
\end{equation}
$$

I kinda hate it because it is a crude approximation that ignores the variance.

For example, imagine you have a normally distributed $q_\theta$ with $\mu=0$. This approximation simply says that $\E_{x \sim q_\theta}[f(x)] = f(0)$. This is an optimistic approximation and it does make sense, but consider the second order approximation $f(0) + \frac{1}{2}\E_{x \sim q_\theta}[\|x\|_{H}^2]$ where $H=\nabla^2 f(0)$. It is straightforward to see (use $\text{Tr}$) that the second term is $\Sigma H$ where $\Sigma = \E_{x \sim q_\theta}[xx^T]$. Thus, the approximation is worse when the curvature of $f$ is the opposite (i.e., inverse) of the covariance.

An extension of the delta method to gradients and Hessians of $f$ is possible as well. For the first, we use Bonnet's Theorem [(Bonnet, 1964)](https://link.springer.com/article/10.1007/BF03014720), which says that $\E_{x \sim q_\theta}[\nabla f(x)] = \nabla_{\mu} \E_{x \sim q_\theta}[f(x)]$. This is only helpful if we can write the inner part in terms of $\mu$, but we can use the delta method and have

$$
\begin{equation}
    \E_{x \sim q_\theta}[\nabla f(x)]
    \overset{(\text{Bonnet})}{=}
    \nabla_{\mu} \E_{x \sim q_\theta}[f(x)]
     \approx \nabla f(\mu).
    \tag{Delta-1}
\end{equation}
$$

As for the Hessian, we use Price's Theorem [(Price, 1958)](https://ieeexplore.ieee.org/document/1057444), which says that $\E_{x \sim q_\theta}[\nabla^2 f(x)] = 2 \nabla_{\Sigma} \E_{x \sim q_\theta}[f(x)]$. This time, we use a second-order approximation $f(\mu) + \frac{1}{2} \Sigma \nabla^2 f(\mu)$ to get

$$
\begin{equation}
    \E_{x \sim q_\theta}[\nabla^2 f(x)]
    \overset{(\text{Price})}{=}
    2 \nabla_{\Sigma} \E_{x \sim q_\theta}[f(x)]
    \approx \nabla^2 f(\mu).
    \tag{Delta-2}
\end{equation}
$$

The three above approximation of the derivatives will be used in the derivation of Newton algorithm from $\text{(BLR-step)}$.

### Newton's Method
Now we have the necessary tools and understanding to derive some practical optimization methods from $\text{(BLR-step)}$. Instead of deriving gradient descent first, we will jump straight to Newton's method, from which it will be easy to see how vanilla gradient descent can be derived.

First, let us write the natural parameters of a Gaussian as $\theta = [\theta^{(1)}; \theta^{(2)}]$ where

$$
\begin{align}
    \theta^{(1)} &= \Sigma^{-1} \mu,
    \\
    \theta^{(2)} &= - \frac{1}{2} \Sigma^{-1}.
\end{align}
$$

Note that we didn't come up with this parameterization. This is the natural parameterization of a (multivariate) Gaussian obtained from writing its distribution as $h(x) \exp (\langle T(x), \theta \rangle - A(\theta))$.

On the other hand, the expectation parameters $\nu(\theta)$ depend on the choice of $T(x)$. Using the straightforward sufficient statistic $T(x) = [x; xx^T]$, we obtain

$$
\begin{align}
    \nu^{(1)} &= \mu
    % &&= \theta^{(1)}
    ,
    \\
    \nu^{(2)} &= \Sigma + \mu\mu^T
    % &&= \theta^{(2)} + \theta^{(1)}\left(\theta^{(1)}\right)^T
    ,
\end{align}
$$

where the second equality follows because $\E[(x-\mu)(x-\mu)^T] = \E[xx^T] - \mu\mu^T$. Note again that $\nu$ depends on $T(x)$ and not on $\theta$.

Now we have the necessary tools to apply BLR. First, recall the BLR step

$$
\begin{equation}
    \theta_{t+1} = \theta_{t} - \eta_t \tilde{\nabla}_{ {\theta_t}} (\E_{x \sim q_{ {\theta_t}} } [f(x) + \Gamma(x)] - H(q_{ {\theta_t}})). \nonumber
\end{equation}
$$

Let $\Gamma(\cdot)=0$ for simplicity. We only have to figure out what $\tilde{\nabla}_{ {\theta_t}} \E_{x \sim q_{ {\theta_t}} } [f(x)]$ is (we already know what the gradient of entropy is given constant base measures as in Gaussians, e.g., see Appendix.)

Remember the chain rule trick we did that says $\tilde{\nabla}_{ {\theta_t}} = \nabla_{ \nu({\theta_t})}$? Great, but now we have *two* partitions that are *not necessarily independent* (with respect to the inherent parameters of the distribution $\mu$ and $\Sigma$). We optimize with respect to these inherent parameters, so we have to write the BLR step in terms of $\mu$ and $\Sigma$.

Let us figure out the gradients $\nabla_{\nu^{(1)}}$ and $\nabla_{\nu^{(2)}}$ first. Let $h(\mu, \Sigma)$ be an arbitrary differentiable function. Then, noting that $\Sigma = \nu^{(2)} - \nu^{(1)} (\nu^{(1)})^T$, we have

$$
\begin{equation}
    \frac{\partial h}{\partial \nu^{(1)}}
    = \frac{\partial h}{\partial \mu} \frac{\partial \mu}{\partial \nu^{(1)}}
        + \frac{\partial h}{\partial \Sigma} \frac{\partial \Sigma}{\partial \nu^{(1)}}
    = \frac{\partial h}{\partial \mu}
        - 2 \frac{\partial h}{\partial \Sigma} \mu,
\end{equation}
$$

and

$$
\begin{equation}
    \frac{\partial h}{\partial \nu^{(2)}}
    = \frac{\partial h}{\partial \Sigma} \frac{\partial \Sigma}{\partial \nu^{(2)}}
    = \frac{\partial h}{\partial \Sigma}.
\end{equation}
$$

Thus,

$$
\begin{align}
    \nabla_{\nu^{(1)}}h(\cdot) &= \nabla_\mu h(\cdot) - 2 \nabla_{\Sigma}h(\cdot) \mu,
    \\
    \nabla_{\nu^{(2)}}h(\cdot) &= \nabla_{\Sigma} h(\cdot)
    .
\end{align}
$$

Using $\text{(Delta-1)}$ and $\text{(Delta-2)}$ the updates and plugging in the gradients in the BLR step, we get

$$
\begin{align}
    \theta_{t+1} &= (1-\eta_t)\theta_{t} - \eta_t \nabla_\nu \E_{x \sim q_{\theta_t}} [f(x)]
    \\ \overset{(1)}{\implies}
    \theta_{t+1}^{(1)} &= (1-\eta_t)\theta_{t}^{(1)} - \eta_t \nabla_\mu \E_{x \sim q_{\theta_t}} [f(x)] + 2\eta_t \nabla_\Sigma \E_{x \sim q_{\theta_t}} [f(x)] \mu_t
    \\
    &= (1-\eta_t)\theta_{t}^{(1)} - \eta_t \nabla f(\mu_t) + \eta_t  \nabla^2 f(\mu_t)\mu_t
    \\ \overset{(2)}{\implies}
    \theta_{t+1}^{(2)} &= (1-\eta_t)\theta_{t}^{(2)} - \eta_t \nabla_\Sigma \E_{x \sim q_{\theta_t}} [f(x)]
    \\
    &= (1-\eta_t)\theta_{t}^{(2)} - \frac{\eta_t}{2} \nabla^2 f(\mu_t)
    .
\end{align}
$$

Substituting the natural parameters of a multivariate Gaussian as defined above, we get

$$
\begin{align}
    \Sigma_{t+1}^{-1} &= (1-\eta_t)\Sigma_{t}^{-1} + \eta_t \nabla^2 f(\mu_t)
    \\
    \mu_{t+1} &= \Sigma_{t+1}\left[
        (1-\eta_t) \Sigma_{t}^{-1}\mu_{t} - \eta_t \nabla f(\mu_t) + \eta_t  \nabla^2 f(\mu_t)\mu_t)
    \right]
    \\ &= \Sigma_{t+1}\left[
        \Sigma_{t+1}^{-1}\mu_{t} - \eta_t \nabla f(\mu_t)
    \right]
    \\ &= \mu_{t} - \eta_t \Sigma_{t+1} \nabla f(\mu_t)
    .
\end{align}
$$

It's crazy how the covariance conveniently factored out and gave us this beautiful algorithm.
Not only did we get a generalized Newton update (where $\eta_t=1$ gives us Newton's method), but we also got an exponentially smoothed Hessian naturally from the theory.

It is straightforward to see how other algorithms derive from this one. For example, gradient descent is a special case when $\Sigma = I$. AdaGrad is a special case when we ignore the entropy of $\Sigma$ and approximate the covariance with the inverse of $\nabla f(\mu_t) \nabla f(\mu_t)^T$. Adam with $\beta_1=0$ is a special case when we use a similar approximation but take the diagonal only. Dropout can also be explained with BLR (See Sec. 4.3 in in the BLR paper). The only interesting case that we didn't cover is momentum. It is possible to derive momentum from BLR, but we will cover it later.

(A note about the diagonal approximation: in practice, we actually estimate the gradient of the minibatch $g=\frac{1}{B} \sum_{i=1}^B \nabla f(\mu_t; \xi_i)$ and then use $\sqrt{g \circ g} + \epsilon$ as the preconditioner. The square root is often taken for granted, but if you look closely, there is a subtle difference between how we approximate the Hessian and what we use as the preconditioner. Namely, $\sqrt{g \circ g} = \frac{1}{B} \sqrt{\sum_{i=1}^B \nabla f(\mu_t; \xi_i) \circ \sum_{i=1}^B \nabla f(\mu_t; \xi_i)}$, whereas the actual approximation should be something like $\frac{1}{B}\sum_{i=1}^B \nabla f(\mu_t; \xi_i) \circ \nabla f(\mu_t; \xi_i)$. This is discussed in more details in a recent paper by [Lin et al. (2024)](https://arxiv.org/abs/2402.03496).)

### Specialized BLR: exploring other special cases of EF distributions
The BLR step above is applied given a multivariate Gaussian distribution on the parameters. Can we derive specialized algorithms for more specialized distributions? Yeah, why not.

#### Bernoulli parameters (aka binary weights)
Binary weights (e.g., -1 and +1 weights) are difficult to train without making some simplifications. In practice, a straight-through estimator is often used for gradients, i.e., using the gradient as if the weights were continuous (and then projecting back to binary space). Modelling the weights as probabilities of Bernoulli distribution is the natural Bayesian approach, and, yes, Bernoulli is an EF distribution, so let us try it out.

First, note the Bernoulli pdf for -1 and +1 samples is $q_{\theta}(x) = p^{b(x)}(1-p)^{1-b(x)}$, where $b(x) = (x+1)/2$ maps -1 and +1 to 0 and 1, respectively.
We want to write the distribution out in EF form in order to find $\theta$

$$
\begin{align}
    \log q_{p}(x) &=
            \begin{pmatrix} b(x) \\ 1-b(x) \end{pmatrix}^T
            \begin{pmatrix} \log p \\ \log(1-p) \end{pmatrix}
    \\
    % &=
    %         \frac{1}{2}\begin{pmatrix} x \\ -x \end{pmatrix}^T
    %         \begin{pmatrix} \log p \\ \log(1-p) \end{pmatrix}
    %         + \log p(1-p)
    % \\
    &=
            x
            \cdot \underbrace{\frac{1}{2} \log \frac{p}{1-p}}_{\theta}
            - \underbrace{\frac{1}{2}\log \frac{1}{p(1-p)}}_{A(\theta)}
    .
\end{align}
$$

Thus, we have $\theta = \frac{1}{2} \log \frac{p}{1-p}$ and $\nu = 2p-1$ since $\E[b(x)]=p$.
We assume that the coordinate of the parameters are independent, i.e., $q_p(x)=\prod_i q_p(x_i)$, so that $T(x) = \sum_i x_i$ is sufficient and the derivation above extends naturally.

Because of the form of $\theta$, it is difficult to compute the update in terms of $p$.
However, note that we can write

$$
\begin{equation}
    p = \exp(2y)/(1+\exp(2y)) = \text{sigmoid}(2y) = (\tanh(y)+1)/2,
\end{equation}
$$

for some real number $y$, so that $\theta = y$.

Indeed, given the function $\delta(\epsilon) = \frac{1}{2} \log \frac{\epsilon}{1-\epsilon}$ and a temperature parameter $\tau > 0$, we can simulate samples from $(-1,+1)$ by a reparameterization that continuizes discrete random variables $\{-1,+1\}$ into what is called as [Concrete random variables](https://arxiv.org/abs/1611.00712) as follows

$$
\begin{equation}
    \tilde{x} = \tanh\left( \frac{\theta + \delta(\epsilon)}{\tau} \right),
    \quad \epsilon \sim \mathcal{U}(0,1).
\end{equation}
$$

Thus, we introduce samples $\tilde{x}$ "in-between" -1 and +1.
Also, as $\tau \to 0$, we get $\tilde{x} \sim \text{sign}(\theta+\delta(\epsilon))$, with an average of $\tanh(\theta)=2p-1$. Thus, this Concrete distribution can be seen as a relaxed (and shifted) case of Bernoulli. 

With that in mind, we approximate $\E_{x \sim q_p} [f(x)] \approx f(\tilde{x})$. Now given the concrete reparameterization, we have $\nu = \tanh(\theta)$ and using the chain rule, we get
$\partial f (\cdot) / \partial \nu_i = (\partial f(\cdot) / \partial \tilde{x}) (\partial \tilde{x} / \partial \theta_i) (\partial \theta_i / \partial \nu_i)$ due to the coordinate-wise relation between $\theta$, $\nu$, and $\tilde{x}$.
Based on this, we calculate

$$
\begin{equation}
    s = \nabla_\nu \theta
    = \frac{1}{\tau} \left[ \frac{1-\tilde{x}^2}{1-\tanh(\theta)^2} \right]
    = \frac{1}{\tau} \left[ \frac{1-\tanh(\frac{\theta + \delta(\epsilon)}{\tau})^2}{1-\tanh(\theta)^2} \right],
\end{equation}
$$

where the operations are done coordinate-wise.
Thus,

$$
\begin{align}
    \theta_{t+1} &= (1-\eta_t)\theta_{t} - \eta_t \nabla_\nu \E_{x \sim q_{\theta_t}} [f(x)],
    \\ \implies
    \theta_{t+1} &= (1-\eta_t) \theta_t - \eta_t s \circ \nabla f(\tilde{x})
    .
\end{align}
$$

The update above is known as BayesBiNN [(Meng et al., 2020)](http://www.arxiv.org/abs/2002.10778), and it recovers the straight-through estimator as a special case when $\tau \to 0$ and the noise is zero (i.e., $\delta(\cdot) = 0$, which is equivalent to applying the delta method).


#### Mixture of Gaussians (aka mixed weights)
This is an interesting case that I found to be relevant to our recent project on [collaborative learning with mixture of adaptors](https://TODO.com).
The FIM of mixture of Gaussians can be singular.
One way to circumvent that is to work directly on the joint distribution with the latent variable (i.e., component indicator variable), which is a minimal-conditional EF distribution [(Lin et al., 2020)](https://arxiv.org/abs/1906.02914).

Given a fixed component, the natural and expectation parameters for a mixture of Gaussians is the same as a regular multivariate Gaussian.
In other words, if $z$ is the variable denoting the component index, then $q_\theta(x|z=k)$ is a multivariate Gaussian by definition.
Consider $q_\theta(x,z=k) = \pi_k q_\theta(x|z=k)$, where $\pi_k = q_\theta(z=k)$.
Since $z$ is a categorical/generalized bernoulli variable, its sufficient statistic is $$\mathbf{1}_{z=k}$$ with expectation $\pi_k$, so the sufficient statistic of $\pi_k q_\theta(x|z=k)$ should be $$\mathbf{1}_{z=k}$$ times the sufficient statistic of $q_\theta(x|z=k)$, which is already known.
Hence,

$$
\begin{align}
    &\theta_k^{(1)}  = \Sigma_k^{-1}\mu_k,
    \quad&
    &\theta_k^{(2)}  = -\frac{1}{2}\Sigma_k^{-1},
    \\
    &T_k^{(1)}(x) = \mathbf{1}_{[z=k]} x,
    \quad&
    &T_k^{(2)}(x) = \mathbf{1}_{[z=k]} xx^T,
    \\
    &\nu_k^{(1)}(x) = \pi_k \mu_k,
    \quad&
    &\nu_k^{(2)}(x) = \pi_k (\Sigma_k + \mu_k\mu_k^T).
\end{align}
$$

These look quite similar to the non-mixture case with an extra term, so how would the updates be different here?

The main difference is that the BLR has an entropy term, so now the BLR step would not exactly have the same exponential smoothing and the cancelling out in the updates.
We should consider the gradient (and Hessian) of the entropy of the mixture as a whole with $\log q_\theta(x) = \log \sum_{i=1}^{K} \pi_k q_\theta(x|z=k)$.
The natural gradients $$\tilde{\nabla}_{\mu_k}$$ and $$\tilde{\nabla}_{\Sigma}$$ of the entropy can be written in terms of $\nabla_{x} \log q_\theta(x)$ and $\nabla_{x}^2 \log q_\theta(x)$ with Bonnet's and Price's theorems, similar to what was done in the multivariate Gaussian case.

Calculating the gradient and Hessian of entropy is tedious, so we moved it to the appendix. In short, we obtain

[TODO]

.


.


.


.


.


.


.


.


.


### BLR as probablistic inference
The BLR can also be used to derive inference algorithms such as expectation-maximization and variational inference.
This is done by setting the loss to be proportional to the negative log-likelihood, i.e.,  $f(x) \propto -\log p(\xi|x)$.
We have already seen how BLR maximizes the ELBO for this loss, for example.
I will not go into detail here, but the interested reader can refer to Section 5 in (Khan & Rue, 2023).


<br>

---

<br>

# Appendix
## Natural gradient of entropy for EF distributions
Recall the natural gradient $$\tilde{\nabla}_\theta = F(\theta)^{-1} \nabla_\theta$$, where $F(\theta)^{-1}$ is the Fisher information matrix. Also, recall the expectation parmeter $\nu(\theta) = \E_{x \sim q_\theta}[T(x)]$.

Assume the exponential family of distributions $q_\theta(x) = h(x) \exp(\langle \theta, T(x) \rangle - A(\theta))$,
so that $F(\theta) = \nabla^2 A(\theta)$ and $\nu(\theta) = \nabla A(\theta)$ (see the main text). Assume further that $h(x)$ is constant.
Then, the natural gradient of negative entropy $-H(q)$ is

$$
\begin{align}
    -\tilde{\nabla}_\theta H(q) &=
    \tilde{\nabla}_\theta \E_{x \sim q_\theta}[\log q_{\theta}(x)]
    \\ &=
    \tilde{\nabla}_\theta \E_{x \sim q_\theta} [\langle \theta, T(x) \rangle - A(\theta)] + \tilde{\nabla}_\theta \E_{x \sim q_\theta}[h(x)]
    \\ &=
    \tilde{\nabla}_\theta [\langle \theta, \nu(\theta) \rangle - A(\theta)]
    \\ &=
    (\nabla_\theta^2 A(\theta))^{-1} (\nabla_\theta \langle \theta, \nu(\theta) \rangle
    - \nabla_\theta A(\theta))
    \\ &=
    (\nabla_\theta^2 A(\theta))^{-1} (\nu(\theta) + \nabla_\theta \nu(\theta) \theta
    - \nabla_\theta A(\theta))
    \\ &=
    (\nabla_\theta^2 A(\theta))^{-1} (\nabla_\theta A(\theta) + \nabla_\theta^2 A(\theta) \theta
    - \nabla_\theta A(\theta))
    \\ &=
    \theta
    .
\end{align}
$$

It is clear from the above that the Euclidean gradient of $-H(q)$ should be $\nabla^2A(\theta) \theta$.

## KL-divergence as Bregman divergence for EF distributions
We have seen that $D_\text{KL} (q_{ {\color{brown} \theta}} \| q_{\theta_t}) = B_{A} (\theta_t, {\color{brown} \theta})$. We want to show that $B_{A} (\theta_t, {\color{brown} \theta}) = B_{A^\ast} (\nu({\color{brown} \theta}) , \nu(\theta_t))$.

First, recall that $\nabla A^\ast = (\nabla A)^{-1}$ and observe that

$$
\begin{equation}
    A^\ast(\nabla A({\color{brown} \theta}))
    = \sup_y \ \langle y, \nabla A({\color{brown} \theta}) \rangle - A(y)
    = \langle {\color{brown} \theta}, \nabla A({\color{brown} \theta}) \rangle - A({\color{brown} \theta}),
\end{equation}
$$

which uniquely holds since $A$ is strictly convex.

Working our way backwards, we have

$$
\begin{align}
    B_{A^\ast} (\nu({\color{brown} \theta}) , \nu(\theta_t))
    &= A^\ast(\nu({\color{brown} \theta})) - A^\ast(\nu(\theta_t)) - \langle \nu({\color{brown} \theta}) - \nu(\theta_t), \nabla A^\ast(\nu(\theta_t)) \rangle
    \\
    &= A^\ast(\nabla A({\color{brown} \theta})) - A^\ast(\nabla A(\theta_t)) - \langle \nabla A({\color{brown} \theta}) - \nabla A(\theta_t), \theta_t \rangle
    \\
    &= \langle {\color{brown} \theta}, \nabla A({\color{brown} \theta}) \rangle - A({\color{brown} \theta}) - \langle \theta_t, \nabla A(\theta_t) \rangle + A(\theta_t) - \langle \nabla A({\color{brown} \theta}) - \nabla A(\theta_t), \theta_t \rangle
    \\
    &= A(\theta_t) - A({\color{brown} \theta}) - \langle \theta_t - {\color{brown} \theta}, \nabla A({\color{brown} \theta}) \rangle
    \\
    &= B_{A} (\theta_t, {\color{brown} \theta}),
\end{align}
$$

which is the identity of interest.

## Natural gradient of entropy for minimal conditional EF distributions (e.g., mixture of Gaussians)
Recall the minimal conditional EF distribution $q_{\theta}(x,z=k) = \pi_k q_{\theta}(x)$, which has the following parameters

$$
\begin{align}
    &\theta_k^{(1)}  = \Sigma_k^{-1}\mu_k,
    \quad&
    &\theta_k^{(2)}  = -\frac{1}{2}\Sigma_k^{-1},
    \\
    &T_k^{(1)}(x) = \mathbf{1}_{[z=k]} x,
    \quad&
    &T_k^{(2)}(x) = \mathbf{1}_{[z=k]} xx^T,
    \\
    &\nu_k^{(1)}(x) = \pi_k \mu_k,
    \quad&
    &\nu_k^{(2)}(x) = \pi_k (\Sigma_k + \mu_k\mu_k^T).
\end{align}
$$

The natural gradients $$\tilde{\nabla}_{\mu_k}$$ and $$\tilde{\nabla}_{\Sigma}$$ of the entropy can be written in terms of $\nabla_{x} \log q_\theta(x)$ and $\nabla_{x}^2 \log q_\theta(x)$ with Bonnet's and Price's theorems, similar to what was done in the multivariate Gaussian case.

First, note that $\log q_\theta(x) = \log \sum_{i=1}^{K} \pi_k q_\theta(x|z=k)$.
We write it as a LogSumExp function since it conveniently has a Softmax gradient.
Namely, let $y_k := \log (\pi_k q_\theta(x|z=k))$ and denote

$$
\begin{equation}
    r_i(x)
    := \frac{\partial}{\partial y_i} \log q_{\theta}(x)
    = \frac{\partial}{\partial y_i} \log \sum_{j=1}^K \exp(y_j)
    = \text{Softmax} (y)_i
    = \frac{\pi_i q_\theta(x|z=i)}{\sum_{j=1}^K \pi_j q_\theta(x|z=j)}
    .
\end{equation}
$$

In fact, it can be easily seen that $r_i(x) = q(z=i|x)$, which is why it is called the *responsibility function*.
Further, denote $s_k(x) = \nabla_x \log q(x,z=k)$, which is the score function of the $k$-th Gaussian, and note that $\nabla_x \log q(x,z=k) = \nabla_x \log q(x|z=k)$ since $\nabla_x \pi_k = 0$.
Then, by the chain rule, we can get

$$
\begin{equation}
    \nabla_{x} \log q_\theta(x) = \sum_{i=1}^K r_i(x) s_i(x),
\end{equation}
$$

which is the score of the mixture.
Interestingly, it can be interpreted as

$$
\begin{equation}
    \nabla_{x} \log q_\theta(x) = \E_{z|x}[s_z(x)] = \E_{z|x}[\nabla_x \log q_\theta(x,z)],
\end{equation}
$$

which is, indeed, the expected score of the mixture.

Next, we derive $\nabla_x^2 \log q_\theta(x)$. First, note the following can be derived easily with the LogSumExp derivative trick

$$
\begin{equation}
    \frac{\partial}{\partial y_j} \text{Softmax}(y)_i = \text{Softmax}(y)_i (\delta_{ij} - \text{Softmax}(y)_j),
\end{equation}
$$

where $\delta_{ij}=1$ if $i=j$, 0 otherwise.
This allows us to obtain $\nabla_x r_i(x) = \sum_{j=1}^K \frac{r_i(y_j)}{\partial y_j} \nabla_x y_j = \sum_{j=1}^K r_i(x) (\delta_{ij}-r_j(x)) s_j(x)$.
Using the Hessian operator $\nabla^2 = \nabla \nabla^T$, we get

$$
\begin{align}
    \nabla^2 \log q_\theta(x)
    &= \sum_{i=1}^K \nabla_x r_i(x) s_i(x)^T + r_i(x) \nabla_x^T s_i(x)
    \\
    &= \sum_{i=1}^K \sum_{j=1}^K r_i(x) (\delta_{ij} -r_{j}(x)) s_{j}(x) s_i(x)^T + r_i(x) \nabla_x^T s_i(x)
    \\
    &= \sum_{i=1}^K r_i(x) \left[
        \nabla_x^T s_i(x)
        + s_i(x) s_i(x)^T
        - \sum_{j=1}^K r_j(x) s_j(x) s_i(x)^T
    \right].
\end{align}
$$

Again, this has an interesting interpretation.
Define the cross-covariance of the scores of the $i$-th and $j$-th Gaussians $C_{ij}(x) = s_i(x) s_j(x)^T$
and the score Jacobian of the $i$-th Gaussian $J_i(x) = \nabla s_i(x)$.
Since the Hessian is symmetric, we can safely transpose all terms,
and recalling that $r_z(x) = q(z|x)$, we can write the Hessian as

$$
\begin{equation}
    \nabla_x^2 \log q_\theta(x)
    = \E_{z|x}[J_z(x) + \hat{C}_{z}(x)],
\end{equation}
$$

where $$\hat{C}_{i}(x) := C_{ii}(x) - \E_{z|x}[C_{iz}(x)]$$ is the centered covariance of the score of the $i$-th Gaussian.
Expanding in detail, we get

$$
\begin{align}
    \nabla_x^2 \log q_\theta(x)
    &= \E_{z|x}[\nabla_x^2 \log q_\theta(x,z) + \nabla_{x} \log q_\theta(x,z) {\color{brown}(\nabla_{x} \log q_\theta(x,z) - \E_{z|x} \nabla_{x} \log q_\theta(x,z))^T}]
    \nonumber
    \\
    &= \E_{z|x}[\nabla_x^2 \log q_\theta(x,z)]
    + \E_{z|x}[\nabla_{x} \log q_\theta(x,z) \nabla_{x} \log q_\theta(x,z)^T]
        - \nabla_{x} \log q_\theta(x) \nabla_{x} \log q_\theta(x)^T
        \nonumber
    ,
\end{align}
$$

so that

$$
\begin{equation}
    \nabla_x^2 \log q_\theta(x)
        + \nabla_{x} \log q_\theta(x) \nabla_{x} \log q_\theta(x)^T
    = \E_{z|x}[\nabla_x^2 \log q_\theta(x,z)]
        + \E_{z|x}[\nabla_{x} \log q_\theta(x,z) \nabla_{x} \log q_\theta(x,z)^T]
    , \nonumber
\end{equation}
$$

which is analogous to the gradient's interpretation.
However, here, we have the Fisher information as well.
This seems like a relaxation of two equalities by summing them together.

Now recall that we can write $\nabla q_\theta(x)$ and $\nabla^2 q_\theta(x)$ in terms of $\mu_k$ and $\Sigma_k$, so we can explicitly calculate the score of a Gaussian.

$$
\begin{equation}
    s_k(x)
    = \nabla_x \langle \theta, T(x) \rangle
    = \nabla_x \langle \Sigma_k^{-1} \mu_k, x \rangle - \frac{1}{2} \nabla_x \text{Tr}(\Sigma_k^{-1} xx^T)
    % = \Sigma_k^{-1} \mu_k - \frac{1}{2} \nabla_x \text{Tr}(x^T\Sigma_k^{-1} x)
    = \Sigma_k^{-1} (\mu_k - x).
    \nonumber
\end{equation}
$$

Thus, we have $J_i(x) = -\Sigma_i^{-1}$ and $C_{ij}(x) = \Sigma_i^{-1} (\mu_i - x) (\mu_j - x)^T \Sigma_j^{-1}$.

<!-- Ideally, we want to have $x_k \approx \mu_k$.
This would imply that $s_k(x_k)=0$ and $\hat{C}_{kj}(x_k) = 0$, $\forall j$.
Thus, in order to have $0 = \nabla_x \log q_\theta(x_k) = \sum_{i \neq k} r_i(x_k) s_i(x_k)$, we need either $r_i(x_k) = 0$ for $i \neq k$ or that the terms cancel out. The second case is impossible in the case of two Gaussians, but one could imagine a scenario for three Gaussians where $r_2(x_1) s_2(x_1) = - r_3(x_1) s_3(x_1)$, for example.
Assuming that $r_i(x_k) = 0$ for $i \neq k$ for simplicity,
we have $\nabla_x \log q_\theta(x_k) = 0$ and $\nabla_x^2 \log q_\theta(x_k) = -\Sigma_k$. -->

<!-- ## Cramer-Rao bound
On a side note, we have a lower bound on the covariance of $T(x)$ using [Cramer-Rao Bound](https://en.wikipedia.org/wiki/Cramér–Rao_bound) (note in the following that $\nabla \nu(\theta)$ is a Jacobian)

$$
\begin{align}
    \text{Cov}_{\theta}[T(x)]
    &\succeq \nabla \nu(\theta) (\nabla^2 A(\theta))^{-1} \nabla \nu(\theta)^T
    = \nabla^2 A(\theta)
    .
\end{align}
$$

Contrast this bound with that of an unbiased estimator $T(x)$, i.e., $\nu = \theta$,

$$
\begin{equation}
    \text{Cov}_{\theta}[T(x)]
    \succeq (\nabla^2 A(\theta))^{-1}.
\end{equation}
$$
 -->



<br>

---

<br>

# References
1. TODO



# Further reading
- [BLR paper](https://arxiv.org/abs/2107.04562)
- [Wu Lin's Blog](https://yorkerlin.github.io/year-archive/)














